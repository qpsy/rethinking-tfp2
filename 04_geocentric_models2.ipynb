{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import arviz as az\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp; tfp = tfp.substrates.jax\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import rethinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rethinking.data import RethinkingDataset\n",
    "from rethinking.data import dataframe_to_tensors\n",
    "from rethinking.mcmc import sample_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to plot the density using arviz plot_kde function\n",
    "def plot_density(positions, x_label=\"Position\", y_label=\"Density\"):\n",
    "    az.plot_kde(positions)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [6, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a key\n",
    "# jax.random.key (new version of jax.random.PRNGKey) \n",
    "#   not working with tfp.substrates.jax.mcmc\n",
    "def a_key():\n",
    "    # return jax.random.key(random.randint(0, 1000_000))\n",
    "    return jax.random.PRNGKey(random.randint(0, 1000_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jnp array replaces tf array\n",
    "# but jnp has yet no equivalant to tf.Variable() \n",
    "from tensorflow_probability.python.internal.backend import jax as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.Variable(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Why normal distributions are normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Normal by addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.1\n",
    "\n",
    "This snippet is showing the notion of \"Normal distribution by Addition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1 = tfd.Uniform(low=-1.0, high=1.0)\n",
    "pos = jnp.sum(u1.sample(sample_shape=(16, 1000), seed=a_key()), axis=0)\n",
    "plot_density(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Normal by multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.2 & 4.3\n",
    "\n",
    "This snippet is showing the notion of \"Normal distribution by Multiplication\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u2 = tfd.Uniform(low=1.0, high=1.1)\n",
    "pos = jnp.prod(u2.sample(sample_shape=(12, 10000), seed=a_key()), axis=0)\n",
    "plot_density(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.4\n",
    "\n",
    "Continuation of the notion of \"Normal distribution by Multiplication\"\n",
    "\n",
    "Author explains that - \"The smaller the effect of a variable (locus in the example), the better the additive approximation will be\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big = jnp.prod(tfd.Uniform(low=1.0, high=1.5).sample(sample_shape=(12, 10000), \n",
    "                                                     seed=a_key()), axis=0)\n",
    "small = jnp.prod(tfd.Uniform(low=1.0, high=1.01).sample(sample_shape=(12, 10000), \n",
    "                                                        seed=a_key()), axis=0)\n",
    "\n",
    "_, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "az.plot_kde(big, ax=ax[0])\n",
    "az.plot_kde(small, ax=ax[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Normal by log-multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.5\n",
    "\n",
    "This snippet is showing the notion of \"Normal distribution by log-multiplication\"\n",
    "\n",
    "Author explains - \"Large deviates that are multi- plied together do not produce Gaussian distributions, but they do tend to produce Gaussian distributions on the log scale. So even multiplicative interactions of large deviations can produce Gaussian distributions, once we measure the outcomes on the log scale.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_big = jnp.log(\n",
    "  jnp.prod(\n",
    "    tfd.Uniform(low=1.0, high=1.5).sample(sample_shape=(12, 10000), \n",
    "                                          seed=a_key()), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "az.plot_kde(big, ax=ax[0])\n",
    "az.plot_kde(log_big, ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 A language for describing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.6\n",
    "\n",
    "Here the globe tossing model is described again.\n",
    "\n",
    "$w$ = Observed count of water\n",
    "\n",
    "$n$ = Total number of tosses\n",
    "\n",
    "$p$ = Proportion of water on the globel\n",
    "\n",
    "\n",
    "This is the first introduction/usage of Bayes' Theorem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, n = 6.0, 9.0\n",
    "\n",
    "p_grid = jnp.linspace(0.0, 1.0, 100)\n",
    "\n",
    "b1_dist = tfd.Binomial(total_count=n, probs=p_grid)\n",
    "u3_dist = tfd.Uniform(low=0.0, high=1.0)\n",
    "\n",
    "posterior = b1_dist.prob(w) * u3_dist.prob(p_grid)\n",
    "posterior = posterior / jnp.sum(posterior)\n",
    "\n",
    "plt.plot(p_grid, posterior)\n",
    "plt.xlabel(\"p\")\n",
    "plt.ylabel(\"Density\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Gaussian model of height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.7, 4.8 \n",
    "\n",
    "\n",
    "We are now building the Gaussian model of height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = RethinkingDataset.Howell1.get_dataset()\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(d.to_dict(orient=\"list\"), kind=\"stats\", hdi_prob=0.89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.10\n",
    "\n",
    "For now we are going to work only with the height columns so let's look at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.height.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.11\n",
    "\n",
    "We are only interested in the heights of adults only (for the time being; we will incorporate the whole dataset later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = d[d.age >= 18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.12 & 4.13\n",
    "\n",
    "Author explains that whatever is your prior, it's a very good idea to always plot it. This way we will have a better\n",
    "sense of our model.\n",
    "\n",
    "Here we have assumed certain values for our priors and as per the instruction plotting them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "x = jnp.linspace(100.0, 250.0, 100)\n",
    "mu_prior = tfd.Normal(loc=178.0, scale=20.0)\n",
    "ax1.set_title(\"Prior for mu\")\n",
    "ax1.plot(x, mu_prior.prob(x))\n",
    "\n",
    "x = jnp.linspace(-10.0, 60.0, 100)\n",
    "scale_prior = tfd.Uniform(low=0.0, high=50.0)\n",
    "ax2.set_title(\"Prior for scale/sigma\")\n",
    "ax2.plot(x, scale_prior.prob(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.14\n",
    "\n",
    "In the above section we have chosen priors for $h$, $\\mu$, $\\sigma$ and even plotted them. It is also important to now see what kind of distribution these assumptions generate. This is called **PRIOR PREDICTIVE simulation** and is important part of modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the joint distribution of h, mu and sigma\n",
    "n_samples = 1000\n",
    "\n",
    "sample_mu = tfd.Normal(loc=178.0, scale=20.0).sample(n_samples, seed=a_key())\n",
    "sample_sigma = tfd.Uniform(low=0.0, high=50.0).sample(n_samples, seed=a_key())\n",
    "prior_h = tfd.Normal(loc=sample_mu, scale=sample_sigma).sample(seed=a_key())\n",
    "\n",
    "az.plot_kde(prior_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.15\n",
    "\n",
    "As mentioned in above cell the prior predictive simulation is helpful in assigning sensible priors. Here we see if we change the scale for *sample_mu* to 100 (instead of using 20 as in the above cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the joint distribution of h, mu and sigma\n",
    "n_samples = 1000\n",
    "\n",
    "sample_mu = tfd.Normal(loc=178.0, scale=100.0).sample(n_samples, seed=a_key())\n",
    "sample_sigma = tfd.Uniform(low=0.0, high=50.0).sample(n_samples, seed=a_key())\n",
    "prior_h = tfd.Normal(loc=sample_mu, scale=sample_sigma).sample(seed=a_key())\n",
    "\n",
    "az.plot_kde(prior_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what the above plot is showing that there are about 4% of people that have a **negative height** and about 18% on the right side have height not common at all for humans to attain ! So clearly this choice of scale (i.e. 100) is not a good one as the prior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Grid approximation of the posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.16\n",
    "\n",
    "Here we are doing **Grid approximation of the posterior distribution**. A very crude technique that may work for small/simple models. It is customary to do it to make a point but later we will use well established inference methods based on MCMC and VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_likelihood(mu, sigma, sample_data):\n",
    "    def _compute(mu, sigma, sample_data):\n",
    "        norm_dist = tfd.Normal(loc=mu, scale=sigma)\n",
    "        return jnp.sum(norm_dist.log_prob(sample_data))\n",
    "\n",
    "    return jax.vmap(_compute, in_axes=(0, 0, None))(mu, sigma, sample_data)\n",
    "\n",
    "\n",
    "def grid_approximation(sample_data, mu_start=150.0, mu_stop=160.0, \n",
    "                       sigma_start=7.0, sigma_stop=9.0):\n",
    "    mu_list = jnp.linspace(start=mu_start, stop=mu_stop, num=200)\n",
    "    sigma_list = jnp.linspace(start=sigma_start, stop=sigma_stop, num=200)\n",
    "\n",
    "    mesh = jnp.meshgrid(mu_list, sigma_list)\n",
    "    mu = jnp.reshape(mesh[0], -1)\n",
    "    sigma = jnp.reshape(mesh[1], -1)\n",
    "\n",
    "    log_likelihood = compute_likelihood(mu, sigma, sample_data)\n",
    "\n",
    "    logprob_mu = tfd.Normal(178.0, 20.0).log_prob(mu)\n",
    "    logprob_sigma = tfd.Uniform(low=0.0, high=50.0).log_prob(sigma)\n",
    "\n",
    "    log_joint_prod = log_likelihood + logprob_mu + logprob_sigma\n",
    "    joint_prob = jnp.exp(log_joint_prod - jnp.max(log_joint_prod))\n",
    "\n",
    "    return mesh, joint_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh, joint_prob = grid_approximation(d2.height.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.17\n",
    "\n",
    "Make a contour plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_joint_prob = jnp.reshape(joint_prob, (200, 200))\n",
    "plt.contour(*mesh, reshaped_joint_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.18\n",
    "\n",
    "Here we are plotting a heatmap instead of contour plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(reshaped_joint_prob, origin=\"lower\", extent=(150, 160, 7, 9), aspect=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4 Sampling from the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.19 & 4.20\n",
    "\n",
    "We are at point of now **Sampling from the Posterior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_prob.shape, joint_prob[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a trick to sample row numbers randomly with the help of Categorical distribution\n",
    "sample_rows = tfd.Categorical(probs=(joint_prob / jnp.sum(joint_prob))).sample(\n",
    "  100_00, seed=a_key())\n",
    "sample_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = mesh[0].reshape(-1)\n",
    "sigma = mesh[1].reshape(-1)\n",
    "\n",
    "# We are sampling 2 parameters here from the selected rows\n",
    "sample_mu = mu[sample_rows]\n",
    "sample_sigma = sigma[sample_rows]\n",
    "\n",
    "plt.scatter(sample_mu, sample_sigma, s=64, alpha=0.1, edgecolor=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.21\n",
    "\n",
    "Time to summarize/describe the distribution of confidence in each combination of $\\mu$ & $\\sigma$ using the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "az.plot_kde(sample_mu, ax=ax[0])\n",
    "ax[0].set_xlabel(\"sample_mu\")\n",
    "ax[0].set_yticks([])\n",
    "az.plot_kde(sample_sigma, ax=ax[1])\n",
    "ax[1].set_xlabel(\"sample_sigma\")\n",
    "ax[1].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As should be clear from the plot, the densities here are very close *normal distribution*. As sample size increases, posterior densities show this behavior. \n",
    "\n",
    "Pay attention to sample_sigma, density of it has a **longer right-hand tail**. Author mentions here that this condition is very common for standard deviation parameters. No explanation about it at this point of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.22\n",
    "\n",
    "Now we want to summarize the widths of these densities with **posterior compatibility intervals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sample_mu), type(sample_mu.__array__()), type(jax.device_get(sample_mu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.5 Finding the posterior distribution using quap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.5 Finding the posterior distribution using quap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sample_mu -\", az.hdi(sample_mu.__array__()))\n",
    "print(\"sample_sigma -\", az.hdi(jax.device_get(sample_sigma)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.23 & Code 4.24\n",
    "\n",
    "Author now wants to use Quadratic approximation but he wants to make a point about using it and impact of $\\sigma$ on this approximation.\n",
    "\n",
    "Here the above analysis of the height data is repeated but with only a fraction of original data. The reasoning behind is to demonstrate that, in principle, the posterior is not always Gaussian in shape.\n",
    "\n",
    "Author explains that for a Guassian $\\mu$ & Gaussian likelihood the shape is always Gaussian irrespective of the sample size but it is the $\\sigma$ that causes problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We just get 20 samples for now and repeat our exercise\n",
    "d3 = d2.height.sample(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh2, joint_prob2 = grid_approximation(\n",
    "    d3.values,\n",
    "    mu_start=150.0,\n",
    "    mu_stop=170.0,\n",
    "    sigma_start=4.0,\n",
    "    sigma_stop=20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu2 = mesh2[0].reshape(-1)\n",
    "sigma2 = mesh2[1].reshape(-1)\n",
    "\n",
    "# We are sampling 2 parameters here from the selected rows\n",
    "sample_mu2 = mu2[sample_rows]\n",
    "sample_sigma2 = sigma2[sample_rows]\n",
    "\n",
    "plt.scatter(sample_mu2, sample_sigma2, s=64, alpha=0.1, edgecolor=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "az.plot_kde(sample_mu2, ax=ax[0])\n",
    "ax[0].set_xlabel(\"sample_mu2\")\n",
    "ax[0].set_yticks([])\n",
    "az.plot_kde(sample_sigma2, ax=ax[1])\n",
    "ax[1].set_xlabel(\"sample_sigma2\")\n",
    "ax[1].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_kde(sample_sigma2)\n",
    "x = jnp.sort(sample_sigma2)\n",
    "plt.plot(x, tfd.Normal(loc=jnp.mean(x), scale=jnp.std(x)).prob(x), \"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.5 Finding the posterior distribution using quap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.26\n",
    "\n",
    "We will reload our data frame and start to use quadratic approximiation now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = RethinkingDataset.Howell1.get_dataset()\n",
    "d2 = d[d.age > 18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.27\n",
    "\n",
    "We are now ready to define our model using tensorflow probability.\n",
    "\n",
    "\n",
    "There are few ways to define a model using tfp, I am going to use a variant\n",
    "of JointSequential that uses Coroutines (generator) based syntax. However, you can always\n",
    "define as a plain function and return log probs yourself. An example of plain function that would be \n",
    "\n",
    "```python\n",
    "def joint_log_prob_model(mu, sigma, data):\n",
    "  mu_dist = tfd.Normal(loc=178., scale=20.)\n",
    "  sigma_dist = tfd.Uniform(low=0., high=50.)\n",
    "  height_dist = tfd.Normal(loc=mu, scale=sigma)\n",
    "  \n",
    "  return (\n",
    "      mu_dist.log_prob(mu) +\n",
    "      sigma_dist.log_prob(sigma) +           \n",
    "      tf.reduce_sum(height_dist.log_prob(data)) \n",
    "  )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = d2.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using JointDistributionNamedAutoBatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m41 = dict(\n",
    "    mu = tfd.Normal(loc=178.0, scale=20.0),\n",
    "    sigma = tfd.Uniform(low=0.0, high=50.0),\n",
    "    height = lambda mu, sigma: tfd.Sample(tfd.Normal(loc=mu, scale=sigma), n_sample)\n",
    ")\n",
    "\n",
    "jd_4_1_N = tfd.JointDistributionNamedAutoBatched(m41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jd_4_1_N.sample(seed=a_key())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample of 'Named': dictionary\n",
    "type(jd_4_1_N.sample(seed=a_key()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mcmc.sample_chain(current_state): Tensor or Python list of Tensor\n",
    "#  - error when dict input \n",
    "# a list of two jnp.array\n",
    "init_state = [jd_4_1_N.sample(seed=a_key())[s] for s in [\"mu\", \"sigma\"]]\n",
    "init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match the function args with the init_state\n",
    "# init_state(a list of two jnp.array) be interpreted as \"mu\" and \"sigma\"\n",
    "def target_log_prob(mu, sigma):\n",
    "    # sample of 'Named': dictionary\n",
    "    samples = {\"mu\": mu, \"sigma\": sigma, \"height\": jnp.array(d2[\"height\"])}\n",
    "    return jd_4_1_N.log_prob(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = tfp.mcmc.HamiltonianMonteCarlo(\n",
    "    target_log_prob_fn=target_log_prob,\n",
    "    step_size=0.2, \n",
    "    num_leapfrog_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = tfp.mcmc.sample_chain(\n",
    "    1000,\n",
    "    current_state=init_state,\n",
    "    kernel=kernel,\n",
    "    num_burnin_steps=200,\n",
    "    trace_fn=None,\n",
    "    seed=a_key())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_dict = dict(zip([\"mu\", \"sigma\"], states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(states_dict, round_to=2, kind=\"stats\", hdi_prob=0.89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of mcmc.sample_chain output depends on shape of init_state\n",
    "init_state = jnp.concatenate(\n",
    "  [jnp.expand_dims(s, axis=0) for s in init_state if jnp.ndim(s)==0])\n",
    "# an jnp.array of two elements\n",
    "init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def target_log_prob(mu, sigma):\n",
    "def target_log_prob(states):\n",
    "    mu, sigma = states\n",
    "    states = {\"mu\": mu, \"sigma\": sigma, \"height\": jnp.array(d2[\"height\"])}\n",
    "    return jd_4_1_N.log_prob(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = tfp.mcmc.HamiltonianMonteCarlo(\n",
    "    target_log_prob_fn=target_log_prob,\n",
    "    step_size=0.2, \n",
    "    num_leapfrog_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = tfp.mcmc.sample_chain(\n",
    "    1000,\n",
    "    current_state=init_state,\n",
    "    kernel=kernel,\n",
    "    num_burnin_steps=200,\n",
    "    trace_fn=None,\n",
    "    seed=a_key())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using JointDistributionCoroutineAutoBatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m41c():\n",
    "    mu = yield tfd.Normal(loc=178.0, scale=20.0, name=\"mu\")\n",
    "    sigma = yield tfd.Uniform(low=0.0, high=50.0, name=\"sigma\")\n",
    "    height = yield tfd.Sample(tfd.Normal(loc=mu, scale=sigma), n_sample, name=\"height\")\n",
    "\n",
    "jd_4_1_C = tfd.JointDistributionCoroutineAutoBatched(m41c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Couroutine' sample: StructTuple\n",
    "init2 = jd_4_1_C.sample(seed=a_key())\n",
    "init2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(init2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seemingly a NamedTuple\n",
    "init2.mu, init2.sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init2[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a NamedTuple for Coroutine.log_prob()\n",
    "StateTuple = namedtuple(\"StateTuple\", [\"mu\", \"sigma\", \"height\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mcmc.sample_chain() can interpret NamedTuple, so two args needed\n",
    "def target_log_prob(mu, sigma):\n",
    "    params = {\"mu\": mu, \"sigma\": sigma, \"height\": jnp.array(d2[\"height\"])}\n",
    "    return jd_4_1_C.log_prob(StateTuple(**params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = tfp.mcmc.HamiltonianMonteCarlo(\n",
    "    target_log_prob_fn=target_log_prob,\n",
    "    step_size=0.2, \n",
    "    num_leapfrog_steps=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init2 = init2[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states2 = tfp.mcmc.sample_chain(\n",
    "    1000,\n",
    "    current_state=init2,\n",
    "    kernel=kernel,\n",
    "    num_burnin_steps=200,\n",
    "    trace_fn=None,\n",
    "    seed=a_key()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(states2._asdict(), round_to=2, kind=\"stats\", hdi_prob=0.89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers shown above provide Gaussian approximations for each parameter’s marginal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a 'jit' version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_chain(init_state, key):\n",
    "#     target_log_prob = target_log_prob(...)\n",
    "#     kernel = tfp.mcmc.HamiltonianMonteCarlo(...)\n",
    "#     return tfp.mcmc.sample_chain(...)\n",
    "\n",
    "# states = jax.jit(run_chain)(init_state, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfd.Sample(tfd.Normal(loc=0., scale=1.), (2,)).sample(seed=a_key())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.array([[3,2]]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.30\n",
    "\n",
    "What should be the starting values for the sampling ?\n",
    "\n",
    "Author recommends that we could use the mean from the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4_2 = dict(\n",
    "    # mu = tfd.Normal(loc=178.0, scale=20.0),\n",
    "    mu = tfd.Normal(loc=178.0, scale=0.1),\n",
    "    sigma = tfd.Uniform(low=0.0, high=50.0),\n",
    "    height = lambda mu, sigma: tfd.Sample(tfd.Normal(loc=mu, scale=sigma), n_sample)\n",
    ")\n",
    "\n",
    "jdc_4_2 = tfd.JointDistributionNamedAutoBatched(model_4_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_burnin_steps = 0\n",
    "num_results = 1000\n",
    "step_size = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jdc_4_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m init_state \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mjdc_4_2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ma_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msigma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[98], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m init_state \u001b[38;5;241m=\u001b[39m [\u001b[43mjdc_4_2\u001b[49m\u001b[38;5;241m.\u001b[39msample(seed\u001b[38;5;241m=\u001b[39ma_key())[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigma\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'jdc_4_2' is not defined"
     ]
    }
   ],
   "source": [
    "init_state = [jdc_4_2.sample(seed=a_key())[s] for s in [\"mu\", \"sigma\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_log_prob(mu, sigma):\n",
    "    samples = {\"mu\": mu, \"sigma\": sigma, \"height\": d2.height.values}\n",
    "    return jdc_4_2.log_prob(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = tfp.mcmc.HamiltonianMonteCarlo(\n",
    "    target_log_prob_fn=target_log_prob,\n",
    "    step_size=step_size, \n",
    "    num_leapfrog_steps=2)\n",
    "\n",
    "kernel = tfp.mcmc.SimpleStepSizeAdaptation(\n",
    "    inner_kernel=kernel, num_adaptation_steps=int(num_burnin_steps * 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, [step_size, log_accept_ratio] = tfp.mcmc.sample_chain(\n",
    "    num_results=num_results,\n",
    "    current_state=init_state,\n",
    "    kernel=kernel,\n",
    "    num_burnin_steps=num_burnin_steps,\n",
    "    trace_fn=lambda _, pkr: [pkr.inner_results.accepted_results.step_size,\n",
    "                             pkr.inner_results.log_accept_ratio],\n",
    "    seed=a_key())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.unique(step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.exp(tfp.math.reduce_logmeanexp(jnp.minimum(log_accept_ratio, 0.)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_dict = dict(zip([\"mu\", \"sigma\"], [jax.device_get(s) for s in states]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(states_dict, round_to=2, kind=\"stats\", hdi_prob=0.89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken verbatim from the book :\n",
    "\n",
    "Notice that the estimate for μ has hardly moved off the prior. The prior was very concentrated around 178. So this is not surprising. But also notice that the estimate for σ has changed quite a lot, even though we didn’t change its prior at all. Once the golem is certain that the mean is near 178—as the prior insists—then the golem has to estimate σ conditional on that fact. This results in a different posterior for σ, even though all we changed is prior information about the other parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = RethinkingDataset.Howell1.get_dataset()\n",
    "d2 = d[d.age > 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = d2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "m41 = dict(\n",
    "    mu = tfd.Normal(loc=178.0, scale=20.0),\n",
    "    sigma = tfd.Uniform(low=0.0, high=50.0),\n",
    "    height = lambda mu, sigma: tfd.Sample(tfd.Normal(loc=mu, scale=sigma), n_sample)\n",
    ")\n",
    "\n",
    "jd_4_1 = tfd.JointDistributionNamedAutoBatched(m41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_name = \"mu\", \"sigma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_log_prob_41(mu, sigma):\n",
    "    sample = dict(zip(state_name, (mu, sigma)), height = d2.height.values)\n",
    "    return jd_4_1.log_prob(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_burnin_steps = 0\n",
    "num_results = 1000\n",
    "step_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_41 = tfp.mcmc.HamiltonianMonteCarlo(\n",
    "    target_log_prob_fn=target_log_prob_41,\n",
    "    step_size=step_size,\n",
    "    num_leapfrog_steps=2)\n",
    "\n",
    "kernel_41 = tfp.mcmc.SimpleStepSizeAdaptation(\n",
    "    inner_kernel=kernel_41, num_adaptation_steps=int(num_burnin_steps * 0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- init_state = [jnp.zeros(()), jnp.zeros(())]\n",
    "  : stay at zero, bad initial\n",
    "- init_state = [jd_4_1.sample((), seed=a_key())[s] for s in state_name]\n",
    "  : a safe way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Array([190.71364, 180.29167, 145.07706], dtype=float32),\n",
       " Array([11.784917 , 14.2883835, 46.93731  ], dtype=float32)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of chains: 3\n",
    "init_state = [jd_4_1.sample((3,), seed=a_key())[s] for s in state_name]\n",
    "init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Array([[185.54544, 177.3295 , 146.23807],\n",
       "        [183.58397, 175.97531, 146.10278],\n",
       "        [182.40666, 175.17046, 146.40485],\n",
       "        ...,\n",
       "        [155.34384, 153.83736, 154.63895],\n",
       "        [154.98505, 154.41971, 154.67397],\n",
       "        [154.96066, 154.4312 , 154.17229]], dtype=float32),\n",
       " Array([[24.588621 , 18.641504 , 46.79445  ],\n",
       "        [25.179623 , 19.712955 , 46.31638  ],\n",
       "        [25.950687 , 20.236235 , 45.260895 ],\n",
       "        ...,\n",
       "        [ 7.6002903,  7.8476787,  8.24986  ],\n",
       "        [ 7.6264834,  7.845391 ,  7.6377115],\n",
       "        [ 7.652736 ,  7.612717 ,  7.894317 ]], dtype=float32)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_41 = tfp.mcmc.sample_chain(\n",
    "    num_results=num_results,\n",
    "    current_state=init_state,\n",
    "    kernel=kernel_41,\n",
    "    num_burnin_steps=num_burnin_steps,\n",
    "    trace_fn=None,\n",
    "    seed=a_key())\n",
    "\n",
    "states_41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_41 = [jnp.moveaxis(s, 0, 1) for s in states_41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_dict = dict(zip(state_name, states_41))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5.5%</th>\n",
       "      <th>hdi_94.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu</th>\n",
       "      <td>154.77</td>\n",
       "      <td>2.27</td>\n",
       "      <td>153.86</td>\n",
       "      <td>155.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma</th>\n",
       "      <td>8.38</td>\n",
       "      <td>3.60</td>\n",
       "      <td>7.24</td>\n",
       "      <td>8.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean    sd  hdi_5.5%  hdi_94.5%\n",
       "mu     154.77  2.27    153.86     155.37\n",
       "sigma    8.38  3.60      7.24       8.29"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stats for all chains\n",
    "az.summary(states_dict, round_to=2, kind=\"stats\", hdi_prob=0.89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5.5%</th>\n",
       "      <th>hdi_94.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu</th>\n",
       "      <td>155.10</td>\n",
       "      <td>3.08</td>\n",
       "      <td>153.93</td>\n",
       "      <td>155.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma</th>\n",
       "      <td>8.18</td>\n",
       "      <td>2.35</td>\n",
       "      <td>7.26</td>\n",
       "      <td>8.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean    sd  hdi_5.5%  hdi_94.5%\n",
       "mu     155.10  3.08    153.93     155.37\n",
       "sigma    8.18  2.35      7.26       8.30"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stats for the first chains\n",
    "# arviz does not \n",
    "az.summary(dict(mu=states_41[0][0], sigma=states_41[1][0]), \n",
    "           round_to=2, kind=\"stats\", hdi_prob=0.89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test az.plot whether it correctly plot each chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code 4.32\n",
    "\n",
    "\n",
    "Now this approximation of posterior is multi-dimension where $\\mu$ and $\\sigma$ are contributing a dimension each. In other words, we have a multi-dimensional gaussian distribution\n",
    "\n",
    "Since it is multi-dimensional, let's see the covariances between the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using only first chain\n",
    "sample_mu, sample_sigma = posterior_4_1[\"mu\"][0], posterior_4_1[\"sigma\"][0]\n",
    "sample_mu, sample_sigma = tf.squeeze(sample_mu), tf.squeeze(sample_sigma)\n",
    "vcov = tfp.stats.covariance(tf.stack([sample_mu, sample_sigma], axis=1))\n",
    "vcov"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfpjax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
